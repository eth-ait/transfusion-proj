<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation.">
  <meta name="keywords" content="ldmoa">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation</title>

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <style type="text/css">
    .publication-authors .author-block sup {
      margin-left: 2px;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-72PW1FZDE4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-72PW1FZDE4');
  </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Razvan Pasca</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a>Alexey Gavryushin</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a>Muhammad Hamza</a><sup>2</sup>,</span><br>
            <span class="author-block">
              <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a><sup>4</sup>,</span>
             <span class="author-block">
              <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html/">Luc Van Gool</a><sup>1,5,6</sup>,</span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a><sup>1</sup>,</span>
             <span class="author-block">
              <a href="https://ait.ethz.ch/people/xiwang/">Xi Wang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> ETH Zurich</span>
            <span class="author-block"><sup>2</sup> Univ. of Zurich</span>
            <span class="author-block"><sup>3</sup> Univ. of Virginia</span>
            <span class="author-block"><sup>4</sup> NVIDIA</span>
            <span class="author-block"><sup>5</sup> KU Leuven</span>
            <span class="author-block"><sup>6</sup> INSAIT, Sofia</span>
            <br>        
            <span class="author-block"> 
              * Equal Contribution</span>
            <br>        
            <span class="author-block"> 
              <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1GNlukUYp0Kxquz3gww1YXOMxtuxI_Mzl/view" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
               <span class="link-block">
                <a href="https://youtu.be/fIIZxHT8q0I" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 
              <!-- Code Link. -->
               <span class="link-block">
                <a href="https://github.com/algvr/transfusion" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span> 
          </div>
        </div>
      </div>
    </div>
</section>

<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="diagram">
        <img src="static/images/ldmoa_teaser.png" alt="Overview" height="750" width="1000" />
    </div>
  </div>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatio-temporal context formed by past actions on objects, coined <i></i>action context</i>. We propose TransFusion, a multimodal transformer-based architecture for short-term object interaction anticipation. Our method exploits the representational power of language by summarizing the action context textually, after leveraging pre-trained vision-language foundation models to extract the action context from past video frames. The summarized action context and the last observed video frame are processed by the multimodal fusion module to forecast the next object interaction. Experiments on the Ego4D next active object interaction dataset show the effectiveness of our multimodal fusion model and highlight the benefits of using the power of foundation models and language-based context summaries in a task where vision may appear to suffice. Our novel approach outperforms the state-of-the-art methods on both versions of the Ego4D dataset.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section_video">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
        <iframe lass="embed-responsive-item "width="960" height="520" src="https://www.youtube.com/embed/fIIZxHT8q0I" frameborder="0" allowfullscreen></iframe> 
        </div>
        <h2 class="subtitle has-text-centered" style="margin-top: 15px;">
            We present a method for next-active-object interaction anticipation using vision-language-modelâ€“generated summaries of the past together with single images.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre>
<code>@inproceedings{
  pasca2024transfusion,
  title={Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation},
  author={Pasca, Razvan and Gavryushin, Alexey and Hamza, Muhammad and Kuo, Yen-Ling and Mo, Kaichun and Van Gool, Luc and Hilliges, Otmar and Wang, Xi},
  booktitle={Conference on Computer Vision and Pattern Recognition 2024},
  year={2024},
  url={https://eth-ait.github.io/transfusion-proj/}
}</code></pre>
  </div>
</section>

</body>
</html>
